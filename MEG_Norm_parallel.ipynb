{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This script reproduces the main analyses and figures in the paper \"MEGaNorm: Normative Modeling of MEG Brain Oscillations Across the Human Lifespan\". We demonstrates how to derive **normative models** of Magnetoencephalography (MEG) recordings and show their application. To do so, we use ([MEGaNorm package](https://pypi.org/project/meganorm/)).\n",
    "\n",
    "The workflow starts with reading MEG data, preprocessing, and extracting relevant features. We then compare two **Hierarchical Bayesian Regression (HBR)** models: **non-linear**, **heteroscedastic**, and **non-Gaussian** model  versus **linear**, **homoscedastic**, and **Gaussian** model. To evaluate these models, we run the normative modeling process **10 times** using different train-test splits (generated with different random seeds), and compute a range of evaluation metrics.\n",
    "\n",
    "After selecting the best-performing model, we demonstrate its predictive power by using it to classify Parkinson's disease. We also explore Parkinson’s disease as a spectrum of abnormalities by analyzing **deviation scores** to decode heterogeneity of this disease. Finally, we estimate normative models on the full dataset and generate **growth charts**, which are later used to create: **Population-based Neuro-Oscillatory Charts (P-NOCs)** and **Individual-based Neuro-Oscillatory Charts (I-NOCs)**.\n",
    "\n",
    "> **Note:** This notebook runs some steps in **parallel**.  \n",
    "> For a sequential version, see `MEG_Norm_sequential.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Workflow\n",
    "\n",
    "1. **Feature extraction.**\n",
    "\n",
    "2. **Comparison of non-linear, heteroscedastic, and non-Gaussian HBR model with linear, homoscedastic, and Gaussian HBR model.**\n",
    "\n",
    "3. **Using deviation scores to detect Parkinson's disease from healthy participants.**\n",
    "\n",
    "4. **Decoding heterogeneity in Parkinson's disease via theta-beta deviation scores.**\n",
    "\n",
    "5. **From growth charts to P-NOCs and I-NOCs.**\n",
    "\n",
    "\n",
    "Each step is explained in detail throughout the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters to Specify\n",
    "\n",
    "- **`datasets`**  \n",
    "  A nested Python dictionary where each key corresponds to a dataset name (e.g., `'BTNRH'`, `'CAMCAN'`, etc.). Each value is a dictionary with the following fields:\n",
    "\n",
    "  - **`base_dir`**:  \n",
    "    The base directory path for the dataset, typically the root folder containing the subject data.\n",
    "\n",
    "  - **`task`**:  \n",
    "    The task label used in the filenames (e.g., `'task-rest'` in `sub-xx_ses-02_task-rest_run-02_meg.ds`). Note that files must be named according to the BIDS format.\n",
    "\n",
    "  - **`ending`**:  \n",
    "    The filename suffix, including its extension (e.g., `'meg.fif'`, `'meg.ds'`, or `'4-Restin/4D'`). Note that files must be named according to the BIDS format.\n",
    "\n",
    "  This structure ensures compatibility across various MEG hardware systems.\n",
    "\n",
    "- **`project_dir`**  \n",
    "  The directory where all output files will be saved.  \n",
    "  **Note:** Use a consistent path across all notebooks to avoid confusion or file overwriting.\n",
    "\n",
    "- **`conda_environment_name`**  \n",
    "  The name of the Python environment used to run the jobs.  \n",
    "  This environment must have the **MEGaNorm** package installed.\n",
    "\n",
    "- **`slurm_username`**  \n",
    "  If using SLURM for parallel computation, provide your SLURM `username`.  \n",
    "  This enables job monitoring and automatic resubmission of incomplete tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'BTNRH': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.fif\"\n",
    "        },\n",
    "    'CAMCAN': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.fif\"\n",
    "        },\n",
    "    'NIMH': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "        },\n",
    "    'OMEGA': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "        },\n",
    "    'HCP': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"4-Restin/4D\"\n",
    "        },\n",
    "    'MOUS': {\n",
    "        'base_dir': ... ,\n",
    "        'task': \"task-rest\",\n",
    "        \"ending\" : \"meg.ds\"\n",
    "    }\n",
    "    }\n",
    "\n",
    "project_dir = ...\n",
    "conda_environment_name = ...\n",
    "slurm_username = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meganorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard library ===\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import tempfile\n",
    "import multiprocessing\n",
    "\n",
    "# === Third-party libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === External tools ===\n",
    "from pcntoolkit.normative_parallel import execute_nm, collect_nm\n",
    "\n",
    "# === meganorm: I/O utilities ===\n",
    "from meganorm.utils.IO import (\n",
    "    merge_fidp_demo,\n",
    "    merge_datasets_with_glob,\n",
    "    factorize_columns,\n",
    "    normalize_column,\n",
    "    separate_patient_data,\n",
    "    make_config\n",
    ")\n",
    "\n",
    "# === meganorm: parallel tools ===\n",
    "from meganorm.utils.parallel import (\n",
    "    submit_jobs,\n",
    "    check_jobs_status,\n",
    "    collect_results,\n",
    "    auto_parallel_feature_extraction\n",
    ")\n",
    "\n",
    "# === meganorm: normative modeling utilities ===\n",
    "from meganorm.utils.nm import (\n",
    "    hbr_data_split,\n",
    "    estimate_centiles,\n",
    "    abnormal_probability,\n",
    "    calculate_PNOCs,\n",
    "    prepare_prediction_data,\n",
    "    cal_stats_for_INOCs,\n",
    "    aggregate_metrics_across_runs,\n",
    ")\n",
    "\n",
    "# === Optional or legacy plotting tools ===\n",
    "from meganorm.plots.plots import (\n",
    "    z_scores_scatter_plot,\n",
    "    plot_INOCs,\n",
    "    plot_metrics,\n",
    "    plot_PNOCs,\n",
    "    qq_plot,\n",
    "    box_plot_auc,\n",
    "    plot_extreme_deviation,\n",
    "    plot_growthcharts\n",
    ")\n",
    "\n",
    "# === Ignore warnings ===\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making the necessary folders and directories to save the features, models and pictures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = os.path.join(project_dir, 'Features')\n",
    "features_log_path = os.path.join(features_dir, 'log')\n",
    "features_temp_path = os.path.join(features_dir,'temp')\n",
    "figures_dir = os.path.join(project_dir, \"figures\")\n",
    "\n",
    "# configs for parallel feature extraction\n",
    "job_configs = {'log_path':features_log_path, 'module':conda_environment_name, 'time':'1:00:00', 'memory':'20GB', \n",
    "                'partition':'normal', 'core':1, 'node':1, 'batch_file_name':'batch_job'}\n",
    "\n",
    "# a log file to save logs of feature extraction\n",
    "if not os.path.isdir(features_log_path):\n",
    "    os.makedirs(features_log_path)\n",
    "\n",
    "# a directory to save extracted features temporarily\n",
    "if not os.path.isdir(features_temp_path):\n",
    "    os.makedirs(features_temp_path)\n",
    "\n",
    "# To save figures\n",
    "if not os.path.isdir(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "# Create a config file that contains all parameters for feature extraction,  \n",
    "# including filtering settings, which features to extract, and other options.\n",
    "configs = make_config(project_dir)\n",
    "\n",
    "subjects = merge_datasets_with_glob(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Feature extraction**\n",
    "The feature extraction process includes reading the data, preprocessing, periodic isolation, feature extraction, and summarization across the brain. This is handled by the `mainParallel` module from the **MEGaNorm** package, which supports both parallel and sequential execution. In this notebook, we use the parallel mode.\n",
    "\n",
    "The features include:\n",
    "- Relative power of theta\n",
    "- Relative power of alpha\n",
    "- Relative power of beta\n",
    "- Relative power of gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_parallel_feature_extraction(\n",
    "    mainParallel_path=os.path.abspath(meganorm.src.mainParallel.__file__),\n",
    "    features_dir=features_dir,\n",
    "    subjects=subjects,\n",
    "    job_configs=job_configs,\n",
    "    config_file=os.path.join(project_dir, 'configuration.json'),\n",
    "    username=slurm_username,\n",
    "    auto_rerun=True,\n",
    "    auto_collect=True,\n",
    "    max_try=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Comparison of non-linear, heteroscedastic, and non-Gaussian HBR model with linear, homoscedastic, and Gaussian HBR model.**\n",
    "\n",
    "Here, we compare two HBR models to select the best one for further analysis. Models are estimated on the training data and evaluated on hold-out test data using a range of performance metrics.\n",
    "\n",
    "To better assess the **generalization error** and **robustness** of the models, this process is repeated **10 times**, each using different train/test splits generated by distinct random seeds. For each split, the feature matrix `X`, target variable `Y`, and batch effect values are saved in a format compatible with **PCNToolkit**. The HBR models are then trained and tested across these runs, and the resulting metrics are stored for visualization.\n",
    "\n",
    "The evaluation metrics include:\n",
    "\n",
    "- **Standardized Mean Squared Error (SMSE)**\n",
    "- **Skewness**\n",
    "- **Excess Kurtosis**\n",
    "- **Shapiro–Wilk test statistic**\n",
    "- **Mean Absolute Centile Error (MACE)**\n",
    "\n",
    "These metrics are visualized to summarize and compare model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Random Seeds\n",
    "\n",
    "To ensure model generalizability, we generate 10 distinct train–test splits by using different random seeds. Below, we define a list of `random_seeds` to create these splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [42, 100, 0, 10, 12, 73, 50, 9, 30, 51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preperation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = os.path.join(project_dir, 'Features')\n",
    "figures_dir = os.path.join(project_dir, \"figures\")\n",
    "nm_processing_dir = os.path.join(project_dir, 'NM', 'Run_' + str(0))\n",
    "\n",
    "### Data preparation for Normative Modeling\n",
    "data_base_dirs = [values[\"base_dir\"] for values in datasets.values()]\n",
    "dataset_names = list(datasets.keys())\n",
    "\n",
    "# Merge demographic data and extracted f-IDPS\n",
    "df = merge_fidp_demo(datasets_paths=data_base_dirs,\n",
    "        features_dir=features_dir,\n",
    "        dataset_names=dataset_names,\n",
    "        drop_columns = [\"eyes\"])\n",
    "\n",
    "# Factorize categorical variables\n",
    "df = factorize_columns(df, \n",
    "        columns=[\"sex\", \"site\"])\n",
    "\n",
    "# Scale age column by dividing by 100\n",
    "df = normalize_column(df,\n",
    "        column=\"age\",\n",
    "        normalizer=100)\n",
    "\n",
    "# Separate patients and healthy participants\n",
    "df_healthy, df_patients = separate_patient_data(df, \n",
    "        diagnosis=[\"parkinson\"])\n",
    "\n",
    "# # Train-Test split for the number of runs\n",
    "for run_counter, run_id in enumerate(random_seeds):\n",
    "        nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{run_counter}\") \n",
    "                \n",
    "        biomarker_names = hbr_data_split(df_healthy, \n",
    "                nm_processing_dir_temp, \n",
    "                drop_nans=True, \n",
    "                batch_effects=['sex', 'site'], \n",
    "                random_seed=run_id, \n",
    "                train_split=0.5, \n",
    "                stratification_columns=[\"site\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HBR Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = os.path.realpath(sys.executable)\n",
    "\n",
    "hbr_configs = {\n",
    "                'homo_Gaussian_linear':{'model_type':'linear', \n",
    "                                        'likelihood':'Normal', \n",
    "                                        'linear_sigma':'False',\n",
    "                                        'random_slope_mu':'False', \n",
    "                                        'linear_epsilon':'False', \n",
    "                                        'linear_delta':'False'}, \n",
    "\n",
    "                'hetero_SHASH_bspline':{'model_type':'bspline', \n",
    "                                        'likelihood':'SHASHb', \n",
    "                                        'linear_sigma':'True',\n",
    "                                        'random_slope_mu':'False', \n",
    "                                        'linear_epsilon':'True', \n",
    "                                        'linear_delta':'True'},\n",
    "            }\n",
    "\n",
    "inscaler='None' \n",
    "outscaler='None' \n",
    "batch_size = 1\n",
    "outputsuffix = '_estimate'\n",
    "\n",
    "memory = '2gb'\n",
    "duration = '5:00:00'\n",
    "cluster_spec = 'slurm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run models n (= lenght of random seeds) times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in hbr_configs.keys():\n",
    "\n",
    "    os.environ[\"PYTENSOR_FLAGS\"] = f\"compiledir={tempfile.mkdtemp(prefix='pytensor_compiledir_')}\"\n",
    "    processes = []\n",
    "\n",
    "    for run_counter, run_id in enumerate(random_seeds):\n",
    "\n",
    "        nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{run_counter}\")\n",
    "\n",
    "        respfile = os.path.join(nm_processing_dir_temp, 'y_train.pkl')\n",
    "        covfile = os.path.join(nm_processing_dir_temp, 'x_train.pkl')\n",
    "        trbefile = os.path.join(nm_processing_dir_temp, 'b_train.pkl')\n",
    "\n",
    "        testrespfile_path = os.path.join(nm_processing_dir_temp, 'y_test.pkl')\n",
    "        testcovfile_path = os.path.join(nm_processing_dir_temp, 'x_test.pkl')\n",
    "        tsbefile = os.path.join(nm_processing_dir_temp, 'b_test.pkl')\n",
    "\n",
    "        processing_dir = os.path.join(nm_processing_dir_temp, method) + '/'\n",
    "        nm_log_path = os.path.join(processing_dir, 'log') + '/'\n",
    "\n",
    "        if not os.path.isdir(processing_dir):\n",
    "            os.makedirs(processing_dir)\n",
    "        if not os.path.isdir(nm_log_path):\n",
    "            os.makedirs(nm_log_path)\n",
    "\n",
    "        p = multiprocessing.Process(target=execute_nm, \n",
    "            args = (processing_dir, \n",
    "                    python_path, \n",
    "                    'NM', \n",
    "                    covfile, \n",
    "                    respfile, \n",
    "                    batch_size, \n",
    "                    memory, \n",
    "                    duration),\n",
    "            kwargs={\n",
    "                'alg': 'hbr',\n",
    "                'log_path': nm_log_path,\n",
    "                'binary': True,\n",
    "                'testcovfile_path': testcovfile_path,\n",
    "                'testrespfile_path': testrespfile_path,\n",
    "                'trbefile': trbefile,\n",
    "                'tsbefile': tsbefile,\n",
    "                'model_type': hbr_configs[method]['model_type'],\n",
    "                'likelihood': hbr_configs[method]['likelihood'],\n",
    "                'linear_sigma': hbr_configs[method]['linear_sigma'],\n",
    "                'random_slope_mu': hbr_configs[method]['random_slope_mu'],\n",
    "                'linear_epsilon': hbr_configs[method]['linear_epsilon'],\n",
    "                'linear_delta': hbr_configs[method]['linear_delta'],\n",
    "                'savemodel': 'True',\n",
    "                'inscaler': inscaler,\n",
    "                'outscaler': outscaler,\n",
    "                'outputsuffix': outputsuffix,\n",
    "                'interactive': 'auto',\n",
    "                'cluster_spec': cluster_spec}\n",
    "            )\n",
    "\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate all the models across runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in hbr_configs.keys():\n",
    "    for run_counter, run_id in enumerate(random_seeds):\n",
    "\n",
    "        nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{run_counter}\")\n",
    "        processing_dir = os.path.join(nm_processing_dir_temp, method) + '/'\n",
    "\n",
    "        collect_nm(processing_dir, \"NM\", collect=True, binary=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calulating metrics across runs and saving them in a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testrespfile_path = os.path.join(nm_processing_dir, 'y_test.pkl')\n",
    "testcovfile_path = os.path.join(nm_processing_dir, 'x_test.pkl')\n",
    "tsbefile = os.path.join(nm_processing_dir, 'b_test.pkl')\n",
    "\n",
    "which_metrics = ['SMSE', 'skewness', 'kurtosis', \"W\", \"MACE\"]\n",
    "\n",
    "for method in hbr_configs.keys():\n",
    "    \n",
    "    metrics_values = aggregate_metrics_across_runs(\n",
    "        path=nm_processing_dir,\n",
    "        method_name=method,\n",
    "        biomarker_names=biomarker_names,\n",
    "        valcovfile_path=testcovfile_path,\n",
    "        valrespfile_path=testrespfile_path,  \n",
    "        valbefile=tsbefile,\n",
    "        metrics = which_metrics,\n",
    "        num_runs = 10,\n",
    "        quantiles = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99],\n",
    "        outputsuffix = \"estimate\",\n",
    "        zscore_clipping_value = 8.0,\n",
    "        )\n",
    "\n",
    "    metrics_summary_path = os.path.join(os.path.join(project_dir, 'NM', f\"metrics_summary_{method}.pkl\"))\n",
    "    with open(metrics_summary_path, \"wb\") as file:\n",
    "        pickle.dump(metrics_values, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot metrics across models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary_path = [os.path.join(project_dir, 'NM', f\"metrics_summary_{method}.pkl\") for method in hbr_configs.keys()]\n",
    "\n",
    "save_metrics_plots = os.path.join(figures_dir, \"model_metrics\")\n",
    "if not os.path.isdir(save_metrics_plots):\n",
    "    os.mkdir(save_metrics_plots)\n",
    "\n",
    "plot_metrics(\n",
    "    metrics_path=metrics_summary_path,\n",
    "    which_biomarkers=biomarker_names,\n",
    "    biomarkers_new_name=[\"Theta\", \"Alpha\", \"Beta\", \"Gamma\"],\n",
    "    colors=[\"teal\", \"orange\"],\n",
    "    save_path=save_metrics_plots,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Q plot on preficted z-scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir_temp = [os.path.join(nm_processing_dir, method) for method in hbr_configs.keys()]\n",
    "\n",
    "save_metrics_plots = os.path.join(figures_dir, \"model_metrics\")\n",
    "if not os.path.isdir(save_metrics_plots):\n",
    "    os.mkdir(save_metrics_plots)\n",
    "\n",
    "label_dict = {\"Theta\": 0, \"Alpha\": 1, \"Beta\": 2, \"Gamma\": 3}\n",
    "qq_plot(processing_dir=processing_dir_temp,\n",
    "        save_fig=save_metrics_plots,\n",
    "        label_dict=label_dict,\n",
    "        colors=[\"orange\", \"teal\", \"olive\", \"tomato\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing the best model \n",
    "\n",
    "Based on the evaluation plots, the **non-linear**, **heteroscedastic**, and **non-Gaussian** HBR model consistently outperforms the baseline (**linear**, **homoscedastic**, and **Gaussian**) model.\n",
    "\n",
    "Therefore, we select the non-linear HBR model as our final model and proceed with it in the subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'hetero_SHASH_bspline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Using deviation scores to detect Parkinson's disease from healthy participants.**\n",
    "\n",
    "After selecting the best model, we use it to detect deviation scores for Parkinson's disease (PD) patients. Specifically, we apply the estimated normative models across runs to compute **deviation scores** for PD participants from the OMEGA dataset, and compare them to healthy participants in the test set of OMEGA.\n",
    "\n",
    "Deviation scores for the healthy participants were already computed in **Step 2**, so we reuse those and only estimate deviation scores for the PD patients.\n",
    "\n",
    "Once all scores are obtained, we compute **AUC (Area Under the Curve)** scores and assess their statistical significance using **permutation testing**, with correction for the **multiple comparisons problem**. Then, AUC values are plotted across **canonical frequency bands** to visualize classification performance. Finally, for further exploration of results, we apply **extreme deviation statistics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test on clinical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seeds = [0]\n",
    "for run_counter, run_id in enumerate(random_seeds):\n",
    "\n",
    "    nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{run_counter}\")\n",
    "    processing_dir_temp = os.path.join(nm_processing_dir_temp, method) + '/'\n",
    "    nm_log_path = os.path.join(processing_dir_temp, 'log') + '/'\n",
    "\n",
    "    prefix = \"clinicalpredict_\"\n",
    "    prepare_prediction_data(df_patients.drop('diagnosis', axis=1),\n",
    "                                nm_processing_dir_temp, \n",
    "                                drop_nans=True, \n",
    "                                batch_effects=['sex', 'site'], \n",
    "                                prefix=prefix)\n",
    "\n",
    "    testrespfile_path = os.path.join(nm_processing_dir_temp, prefix + 'y_test.pkl')\n",
    "    testcovfile_path = os.path.join(nm_processing_dir_temp, prefix + 'x_test.pkl')\n",
    "    tsbefile = os.path.join(nm_processing_dir_temp, prefix + 'b_test.pkl')\n",
    "\n",
    "    execute_nm(processing_dir_temp, \n",
    "    python_path,\n",
    "            'NM', \n",
    "            testcovfile_path, \n",
    "            testrespfile_path, \n",
    "            batch_size, \n",
    "            memory, \n",
    "            duration, \n",
    "            alg='hbr', \n",
    "            log_path=nm_log_path, \n",
    "            binary=True, \n",
    "            tsbefile=tsbefile, \n",
    "            func=\"predict\", \n",
    "            model_type=hbr_configs[method]['model_type'], \n",
    "            likelihood=hbr_configs[method]['likelihood'], \n",
    "            linear_sigma=hbr_configs[method]['linear_sigma'], \n",
    "            random_slope_mu=hbr_configs[method]['random_slope_mu'],\n",
    "            linear_epsilon=hbr_configs[method]['linear_epsilon'], \n",
    "            linear_delta=hbr_configs[method]['linear_delta'], \n",
    "            savemodel='True', \n",
    "            inscaler=inscaler, \n",
    "            outscaler=outscaler, \n",
    "            outputsuffix=\"clinicalpredict\", \n",
    "            inputsuffix=outputsuffix,\n",
    "            interactive='auto', \n",
    "            cluster_spec=cluster_spec, \n",
    "            nuts_sampler=\"nutpie\", \n",
    "            n_cores_per_batch=\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating AUC scores and their significance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_site_id = np.where(np.asarray(dataset_names) == \"OMEGA\")[0].item()\n",
    "p_vals, aucs = [], []\n",
    "\n",
    "for i in range(5, len(random_seeds)):\n",
    "\n",
    "    nm_processing_dir_temp = nm_processing_dir.replace(\"Run_0\", f\"Run_{i}\")\n",
    "    processing_dir_temp = os.path.join(nm_processing_dir_temp, method) + '/'\n",
    "\n",
    "    p_val, auc = abnormal_probability(processing_dir_temp,\n",
    "                                    nm_processing_dir_temp, \n",
    "                                    n_permutation=1000,\n",
    "                                    site_id = omega_site_id,\n",
    "                                    healthy_data_prefix = \"estimate\",\n",
    "                                    patient_data_prefix = \"clinicalpredict\")\n",
    "    \n",
    "    p_vals.append(p_val); aucs.append(auc)\n",
    "\n",
    "p_vals = pd.DataFrame(np.vstack(p_vals))\n",
    "aucs = pd.DataFrame(np.vstack(aucs))\n",
    "\n",
    "aucs.columns = biomarker_names\n",
    "p_vals.columns = biomarker_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot of AUCs across biomarkers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_auc(aucs, \n",
    "    save_path=figures_dir,\n",
    "    color=[\"orange\", \"teal\", \"olive\", \"tomato\"],\n",
    "    alpha=0.7,\n",
    "    biomarkers_new_name=[\"Theta\", \"Alpha\", \"Beta\", \"Gamma\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extreme deviation statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir_temp = os.path.join(nm_processing_dir, method) \n",
    "\n",
    "df_c_pos, df_p_pos, df_c_neg, df_p_neg = plot_extreme_deviation(\n",
    "                    base_path=nm_processing_dir, \n",
    "                    len_runs=10,\n",
    "                    save_path=figures_dir,\n",
    "                    healthy_prefix=\"estimate\", \n",
    "                    site_id = [omega_site_id],\n",
    "                    legend=[\"PD\", \"control\"],\n",
    "                    patient_prefix=\"clinicalpredict\",\n",
    "                    method=method,\n",
    "                    new_col_name=['Theta', 'Alpha', 'Beta', 'Gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Decoding heterogeneity in Parkinson's disease via theta-beta deviation profiles.**\n",
    "\n",
    "In this section, we explore Parkinson's disease from a dimensional perspective by plotting the joint distribution of theta and beta deviation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir_temp = os.path.join(nm_processing_dir, method) + '/'\n",
    "with open(os.path.join(processing_dir_temp, \"Z_clinicalpredict.pkl\"), \"rb\") as file:\n",
    "    z_patient = pickle.load(file)\n",
    "    z_patient.index = df_patients.index\n",
    "    z_patient.columns = biomarker_names\n",
    "\n",
    "\n",
    "z_scores_scatter_plot(X = list(z_patient.loc[:, \"Adjusted_Canonical_Relative_PowerGamma_all\"]),\n",
    "                    Y = list(z_patient.loc[:, \"Adjusted_Canonical_Relative_PowerBeta_all\"]),\n",
    "                    bands_name=[\"Gamma\", \"Beta\"], \n",
    "                    lower_lim = -4.2,\n",
    "                    upper_lim = 4.2,\n",
    "                    ticks = [-4, -2, 0, 2, 4],\n",
    "                    save_path=features_dir\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **From growth charts to P-NOCs and I-NOCs.**\n",
    "\n",
    "For the final step, we fit our proposed HBR models to the full dataset of healthy participants. From these normative models, we derive **growth charts** that capture the trajectory of spectral‐power changes across the lifespan. These charts then serve as the basis for generating both **Population‐level Neuro‑Oscillo Charts (P‑NOCs)** and **Individual‑level Neuro‑Oscillo Charts (I‑NOCs)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimating Normative Models on the Full Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_processing_dir_all = os.path.join(project_dir, 'NM', 'Run_all')\n",
    "if not os.path.isdir(nm_processing_dir_all):\n",
    "        os.mkdir(nm_processing_dir_all)\n",
    "\n",
    "biomarker_names = hbr_data_split(df_healthy,\n",
    "        nm_processing_dir_all,\n",
    "        drop_nans=True,\n",
    "        batch_effects=['sex', 'site'],\n",
    "        random_seed=run_id,\n",
    "        train_split=0.99,\n",
    "        stratification_columns=[\"site\"])\n",
    "\n",
    "outputsuffix = '_estimateAll'\n",
    "\n",
    "respfile = os.path.join(nm_processing_dir_all, 'y_train.pkl')\n",
    "covfile = os.path.join(nm_processing_dir_all, 'x_train.pkl')\n",
    "trbefile = os.path.join(nm_processing_dir_all, 'b_train.pkl')\n",
    "\n",
    "testrespfile_path = os.path.join(nm_processing_dir_all, 'y_test.pkl')\n",
    "testcovfile_path = os.path.join(nm_processing_dir_all, 'x_test.pkl')\n",
    "tsbefile = os.path.join(nm_processing_dir_all, 'b_test.pkl')\n",
    "\n",
    "processing_dir = os.path.join(nm_processing_dir_all, method) + '/'\n",
    "nm_log_path = os.path.join(processing_dir, 'log') + '/'\n",
    "\n",
    "if not os.path.isdir(processing_dir):\n",
    "    os.makedirs(processing_dir)\n",
    "if not os.path.isdir(nm_log_path):\n",
    "    os.makedirs(nm_log_path)\n",
    "\n",
    "\n",
    "execute_nm(processing_dir, \n",
    "            python_path,\n",
    "            'NM', \n",
    "            covfile, \n",
    "            respfile, \n",
    "            batch_size, \n",
    "            memory, \n",
    "            duration, \n",
    "            alg='hbr', \n",
    "            log_path=nm_log_path, \n",
    "            binary=True, \n",
    "            testcovfile_path=testcovfile_path, \n",
    "            testrespfile_path=testrespfile_path,trbefile=trbefile, \n",
    "            tsbefile=tsbefile, \n",
    "            model_type=hbr_configs[method]['model_type'], \n",
    "            likelihood=hbr_configs[method]['likelihood'], \n",
    "            linear_sigma=hbr_configs[method]['linear_sigma'], \n",
    "            random_slope_mu=hbr_configs[method]['random_slope_mu'],\n",
    "            linear_epsilon=hbr_configs[method]['linear_epsilon'], \n",
    "            linear_delta=hbr_configs[method]['linear_delta'], \n",
    "            savemodel='True', \n",
    "            inscaler=inscaler, \n",
    "            outscaler=outscaler, \n",
    "            outputsuffix=outputsuffix, \n",
    "            interactive='auto', \n",
    "            cluster_spec=cluster_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir_temp = os.path.join(nm_processing_dir_all, method) + '/'\n",
    "\n",
    "collect_nm(processing_dir_temp, \"NM\", collect=True, binary=True, batch_size=1, outputsuffix=outputsuffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate centiles of variation for models, derived from the full dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsuffix='estimateAll'\n",
    "# Plotting ranges\n",
    "processing_dir_temp = os.path.join(nm_processing_dir_all, method)\n",
    "\n",
    "q = estimate_centiles(processing_dir_temp, \n",
    "                    bio_num= len(biomarker_names), \n",
    "                    quantiles=[0.05, 0.25, 0.5, 0.75, 0.95],\n",
    "                    age_range=[6, 80],\n",
    "                    outputsuffix=outputsuffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Growth charts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "processing_dir_temp = os.path.join(nm_processing_dir_all, method) \n",
    "\n",
    "plot_growthcharts(processing_dir_temp, \n",
    "                model_indices = list(range(len(new_names))), \n",
    "                biomarker_names = new_names, \n",
    "                site = None, \n",
    "                point_num  = 100, \n",
    "                number_of_sexs = 2, \n",
    "                num_of_sites = 6,\n",
    "                centiles_name = ['5th', '25th', '50th', '75th', '95th'],\n",
    "                colors = {\"male\": ['#4c061d', '#662333', '#803449', '#993d5e', '#b34e74'],\n",
    "                        \"female\": [\"#FF6F00\", \"#FF8C1A\", \"#FFA726\", \"#FFB74D\", \"#FFD54F\"]},\n",
    "                suffix=outputsuffix,\n",
    "                save_path=features_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Individual NeuroOscilloCharts (I-NOCs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir_temp = os.path.join(nm_processing_dir_all, method) \n",
    "q_path = os.path.join(processing_dir_temp, \n",
    "                    f\"Quantiles_{outputsuffix}.pkl\")\n",
    "\n",
    "sub_index = ...\n",
    "statistics = cal_stats_for_INOCs(q_path, \n",
    "                                biomarker_names, \n",
    "                                site_id=df_patients.loc[sub_index][\"site\"], \n",
    "                                sex_id=df_patients.loc[sub_index][\"sex\"], \n",
    "                                age=df_patients.loc[sub_index][\"age\"]*100,\n",
    "                                num_of_datasets=len(datasets.keys()))\n",
    "\n",
    "for i, name in enumerate(biomarker_names):\n",
    "\n",
    "    if biomarker_names[i] == \"Gamma\": \n",
    "        max_value=0.2\n",
    "    else: max_value=1\n",
    "\n",
    "    plot_INOCs(sub_index = sub_index,\n",
    "                observed_value = df_patients.loc[sub_index, name],\n",
    "                q1 = statistics[name][1],\n",
    "                q3 = statistics[name][3],\n",
    "                percentile_5 = statistics[name][0],\n",
    "                percentile_95 = statistics[name][4],\n",
    "                percentile_50 = statistics[name][2],\n",
    "                title=\"\",\n",
    "                max_value=max_value,\n",
    "                show_legend=False,\n",
    "                bio_name=[\"Theta\", \"Alpha\", \"Beta\", \"Gamma\"][i],\n",
    "                save_path=os.path.join(features_dir, \"inocs_for_z_scores\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Population NeuroOscilloCharts (P-NOCs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculateing Oscilograms\n",
    "processing_dir_temp = os.path.join(nm_processing_dir_all, method) \n",
    "gender_ids = {'Male':0, 'Female':1}\n",
    "frequency_band_model_ids = {'Theta':0, 'Alpha':1, 'Beta':2, 'Gamma':3}\n",
    "quantiles_path= os.path.join(processing_dir_temp, f'Quantiles_{outputsuffix}.pkl')\n",
    "oscilograms, age_slices = calculate_PNOCs(quantiles_path, gender_ids, frequency_band_model_ids, num_of_datasets=len(datasets.keys()))\n",
    "\n",
    "plot_PNOCs(oscilograms, age_slices, save_path=features_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
